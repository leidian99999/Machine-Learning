特征工程

来源：
1.业务已经整理好各种特征数据，我们需要去找出适合我们问题需要的特征
2.从业务特征中自己去寻找高级数据特征

选择合适的特征：

###############特征选择###############

一、过滤法选择特征
1.方差筛选：越大越有用。
sklearn中的VarianceThreshold类可以很方便的完成这个工作。

2.相关系数：主要用于输出连续值的监督学习算法中。分别计算所有训练集中各个特征与输出值之间的相关系数，设定一个阈值，选择相关系数较大的部分特征。

3.假设检验：卡方检验，检验某个特征分布和输出值分布之间的相关性。（优先）
sklearn的chi2类，得到所有特征的卡方值与显著性水平P临界值，按阈值选较大的特征

F检验和t检验，它们都是使用假设检验的方法，
只是使用的统计分布不是卡方分布，而是F分布和t分布而已。
在sklearn中，有F检验的函数f_classif和f_regression

4.互信息（信息增益）（优先）：即从信息熵的角度分析各个特征和输出值之间的关系评分。
互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留。
sklearn中，mutual_info_classif(分类)和mutual_info_regression(回归)

二、包装法选择特征(选择一个目标函数来一步步的筛选特征)from sklearn.feature_selection import RFE
递归消除特征法:(recursive feature elimination,以下简称RFE)
使用一个机器学习模型来进行多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进行下一轮训练。
在sklearn中，可以使用RFE函数来选择特征。

三、嵌入法选择特征 from sklearn.feature_selection import SelectFromModel
它和RFE的区别是它不是通过不停的筛掉特征来进行训练，而是使用的都是特征全集
在sklearn中，使用SelectFromModel函数来选择特征。

L1正则化和L2正则化：去掉先变0的
正则化惩罚项越大，那么模型的系数就会越小。
当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0；
当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0。
但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，我们选择特征系数较大的特征。
常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归。也可以DT或GBDT。可以得到特征系数coef或者可以得到特征重要度(feature importances)的算法才可以做为嵌入法的基学习器。

三、挖掘高级特征
在第一次建立模型的时候，我们可以先不寻找高级特征，得到以后基准模型后，再寻找高级特征进行优化。

若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。
若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。
若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。
若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。

聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。（经验）

##############特征处理##############

一、缺失值处理：先看连续值还是离散值。
连续值：均值or中位数填充
离散值：出现频数最高的类别进行填充
from sklearn.preprocessing import Imputer

二、特殊处理
1.时间特征：
a.变连续变量：计算出所有样本的时间到某一个未来时间之间的数值差距，这样这个差距是UTC的时间差，从而将时间特征转化为连续值
b.变离散变量：根据时间所在的年，月，日，星期几，小时数，将一个时间特征转化为若干个离散特征，这种方法在分析具有明显时间趋势的问题比较好用
c.权重法：根据时间衰变给与权重

2.地理特征：靠API解析

3.离散变量连续化：线性回归，逻辑回归
a.OneHotEncoder
b.embedding(深度学习)tf.nn.embedding_lookup
c.在自然语言处理中，用word2vec将词转化为词向量

4.离散变量离散化（高中低）
a.OneHotEncoder (常用)
b.dummy coding

5.连续变量离散化（可使特征变得高维稀疏，方便一些算法的处理。）
a.常用通过阈值分组
b.LR+GBDT(还没学)

sklearn.GradientBoostingClassifier

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder
X, y = make_classification(n_samples=10)  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)
gbc = GradientBoostingClassifier(n_estimators=2)
one_hot = OneHotEncoder()
gbc.fit(X_train, y_train)
X_train_new = one_hot.fit_transform(gbc.apply(X_train)[:, :, 0])
print (X_train_new.todense())


################特征预处理###################

一、标准化和归一化（一个意思）
1.z-score:最常见，基本所有线性模型都会用，
a.用（x-mean)/std来代替原特征。这样特征就变成了均值为0，方差为1了。
sklearn.preprocessing.scale()
StandardScaler

2.max-min:离差标准化，预处理后使特征值映射到[0,1]之间。
a.(x-min)/(max-min)来代替原特征
b.我们希望将数据映射到任意一个区间[a,b]，而不是[0,1]。用(x-min)(b-a)/(max-min)+a
c.除非对特征的取值区间有需求，否则max-min标准化没有 z-score标准化好用。
MinMaxScaler

3.L1/L2范数:
a.如果只是为了统一量纲，通过L2范数整体标准化就行
b.求出每个样本特征向量x⃗ 的L2范数||x⃗ ||2,然后用x⃗ /||x⃗ ||2代替原样本特征即可
c.L1通L2
d.通常情况下，范数标准化首选L2范数标准化。
Normalizer

4.中心化
在PCA降维的时候，用x-mean代替原特征，也就是特征的均值变成了0, 但是方差并不改变。因为PCA就是依赖方差来降维的。

PS:大部分机器学习模型都需要做标准化和归一化，也有的模型可以不做，主要是基于概率分布的模型，比如决策树大家族的CART，随机森林等。当然做了也会对泛化能力也有改进。

二、异常特征样本清洗

1.聚类：KMeans聚类将训练样本分成若干个簇，
如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。过滤。

2.异常点检测方法：iForest或者one class SVM

三、处理不平衡数据

1.权重法：
a.对训练集里的每个类别加一个权重class weight。如果该类别样本数多，那么它的权重就低，反之则权重就高。
b.还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。
sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用。

如果效果不好，用采样法

2.采样法
a.子采样：对类别样本数多的样本做子采样,A占90%，B占10%,对A子采样.直到子采样得到的A类样本数 = B类别现有样本
b.过采样：对B做过采样，直到过采样得到的B类别样本数加上B类别原来样本一起和A类样本数一致
以上方法可能改变数据分布，降低泛化能力

c.SMOTE算法:通过人工合成的方法来生成少类别的样本
对于某一个缺少样本的类别，它会随机找出几个该类别的样本,再找出最靠近这些样本的若干个该类别样本，组成一个候选合成集合，然后在这个集合中不停的选择距离较近的两个样本，在这两个样本之间，比如中点，构造一个新的该类别样本。
(x1,y),(x2,y) ==> ((x1+x2)/2,y)

imbalance-learn.SMOTEENN

