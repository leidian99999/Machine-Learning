{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scikit-learn中有两类决策树，它们均采用优化的CART决策树算法。\n",
    "'''\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "'''\n",
    "回归决策树\n",
    "'''\n",
    "DecisionTreeRegressor(criterion=\"mse\",\n",
    "                         splitter=\"best\",\n",
    "                         max_depth=None,\n",
    "                         min_samples_split=2,\n",
    "                         min_samples_leaf=1,\n",
    "                         min_weight_fraction_leaf=0.,\n",
    "                         max_features=None,\n",
    "                         random_state=None,\n",
    "                         max_leaf_nodes=None,\n",
    "                         min_impurity_decrease=0.,\n",
    "                         min_impurity_split=None,\n",
    "                         presort=False)\n",
    "'''\n",
    "参数含义：\n",
    "1.criterion:string, optional (default=\"mse\")\n",
    "            它指定了切分质量的评价准则。默认为'mse'(mean squared error)。\n",
    "2.splitter:string, optional (default=\"best\")\n",
    "            它指定了在每个节点切分的策略。有两种切分策咯：\n",
    "            (1).splitter='best':表示选择最优的切分特征和切分点。\n",
    "            (2).splitter='random':表示随机切分。\n",
    "3.max_depth:int or None, optional (default=None)\n",
    "             指定树的最大深度。如果为None，则表示树的深度不限，直到\n",
    "             每个叶子都是纯净的，即叶节点中所有样本都属于同一个类别，\n",
    "             或者叶子节点中包含小于min_samples_split个样本。\n",
    "4.min_samples_split:int, float, optional (default=2)\n",
    "             整数或者浮点数，默认为2。它指定了分裂一个内部节点(非叶子节点)\n",
    "             需要的最小样本数。如果为浮点数(0到1之间)，最少样本分割数为ceil(min_samples_split * n_samples)\n",
    "5.min_samples_leaf:int, float, optional (default=1)\n",
    "             整数或者浮点数，默认为1。它指定了每个叶子节点包含的最少样本数。\n",
    "             如果为浮点数(0到1之间)，每个叶子节点包含的最少样本数为ceil(min_samples_leaf * n_samples)\n",
    "6.min_weight_fraction_leaf:float, optional (default=0.)\n",
    "             它指定了叶子节点中样本的最小权重系数。默认情况下样本有相同的权重。\n",
    "7.max_feature:int, float, string or None, optional (default=None)\n",
    "             可以是整数，浮点数，字符串或者None。默认为None。\n",
    "             (1).如果是整数，则每次节点分裂只考虑max_feature个特征。\n",
    "             (2).如果是浮点数(0到1之间)，则每次分裂节点的时候只考虑int(max_features * n_features)个特征。\n",
    "             (3).如果是字符串'auto',max_features=n_features。\n",
    "             (4).如果是字符串'sqrt',max_features=sqrt(n_features)。\n",
    "             (5).如果是字符串'log2',max_features=log2(n_features)。\n",
    "             (6).如果是None，max_feature=n_feature。\n",
    "8.random_state:int, RandomState instance or None, optional (default=None)\n",
    "             (1).如果为整数，则它指定了随机数生成器的种子。\n",
    "             (2).如果为RandomState实例，则指定了随机数生成器。\n",
    "             (3).如果为None，则使用默认的随机数生成器。\n",
    "9.max_leaf_nodes:int or None, optional (default=None)\n",
    "             (1).如果为None，则叶子节点数量不限。\n",
    "             (2).如果不为None，则max_depth被忽略。\n",
    "10.min_impurity_decrease:float, optional (default=0.)\n",
    "             如果节点的分裂导致不纯度的减少(分裂后样本比分裂前更加纯净)大于或等于min_impurity_decrease，则分裂该节点。\n",
    "             个人理解这个参数应该是针对分类问题时才有意义。这里的不纯度应该是指基尼指数。\n",
    "             回归生成树采用的是平方误差最小化策略。分类生成树采用的是基尼指数最小化策略。\n",
    "             加权不纯度的减少量计算公式为：\n",
    "             min_impurity_decrease=N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "             其中N是样本的总数，N_t是当前节点的样本数，N_t_L是分裂后左子节点的样本数，\n",
    "             N_t_R是分裂后右子节点的样本数。impurity指当前节点的基尼指数，right_impurity指\n",
    "             分裂后右子节点的基尼指数。left_impurity指分裂后左子节点的基尼指数。\n",
    "11.min_impurity_split:float\n",
    "             树生长过程中早停止的阈值。如果当前节点的不纯度高于阈值，节点将分裂，否则它是叶子节点。\n",
    "             这个参数已经被弃用。用min_impurity_decrease代替了min_impurity_split。\n",
    "12.presort： bool, optional (default=False)\n",
    "             指定是否需要提前排序数据从而加速寻找最优切分的过程。设置为True时，对于大数据集\n",
    "             会减慢总体的训练过程；但是对于一个小数据集或者设定了最大深度的情况下，会加速训练过程。\n",
    "属性：\n",
    "1.feature_importances_ : array of shape = [n_features]\n",
    "             特征重要性。该值越高，该特征越重要。\n",
    "             特征的重要性为该特征导致的评价准则的（标准化的）总减少量。它也被称为基尼的重要性\n",
    "2.max_feature_:int\n",
    "             max_features推断值。\n",
    "3.n_features_：int\n",
    "             执行fit的时候，特征的数量。\n",
    "4.n_outputs_ : int\n",
    "             执行fit的时候，输出的数量。\n",
    "5.tree_ : 底层的Tree对象。\n",
    "Notes：\n",
    "控制树大小的参数的默认值（例如``max_depth``，``min_samples_leaf``等）导致完全成长和未剪枝的树，\n",
    "这些树在某些数据集上可能表现很好。为减少内存消耗，应通过设置这些参数值来控制树的复杂度和大小。\n",
    "方法：\n",
    "1.fit(X,y):训练模型。\n",
    "2.predict(X):预测。\n",
    "'''\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "'''\n",
    "分类决策树\n",
    "'''\n",
    "DecisionTreeClassifier(criterion=\"gini\",\n",
    "                 splitter=\"best\",\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.,\n",
    "                 max_features=None,\n",
    "                 random_state=None,\n",
    "                 max_leaf_nodes=None,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None,\n",
    "                 class_weight=None,\n",
    "                 presort=False)\n",
    "'''\n",
    "参数含义：\n",
    "1.criterion:string, optional (default=\"gini\")\n",
    "            (1).criterion='gini',分裂节点时评价准则是Gini指数。\n",
    "            (2).criterion='entropy',分裂节点时的评价指标是信息增益。\n",
    "2.max_depth:int or None, optional (default=None)。指定树的最大深度。\n",
    "            如果为None，表示树的深度不限。直到所有的叶子节点都是纯净的，即叶子节点\n",
    "            中所有的样本点都属于同一个类别。或者每个叶子节点包含的样本数小于min_samples_split。\n",
    "3.splitter:string, optional (default=\"best\")。指定分裂节点时的策略。\n",
    "           (1).splitter='best',表示选择最优的分裂策略。\n",
    "           (2).splitter='random',表示选择最好的随机切分策略。\n",
    "4.min_samples_split:int, float, optional (default=2)。表示分裂一个内部节点需要的做少样本数。\n",
    "           (1).如果为整数，则min_samples_split就是最少样本数。\n",
    "           (2).如果为浮点数(0到1之间)，则每次分裂最少样本数为ceil(min_samples_split * n_samples)\n",
    "5.min_samples_leaf: int, float, optional (default=1)。指定每个叶子节点需要的最少样本数。\n",
    "           (1).如果为整数，则min_samples_split就是最少样本数。\n",
    "           (2).如果为浮点数(0到1之间)，则每个叶子节点最少样本数为ceil(min_samples_leaf * n_samples)\n",
    "6.min_weight_fraction_leaf:float, optional (default=0.)\n",
    "           指定叶子节点中样本的最小权重。\n",
    "7.max_features:int, float, string or None, optional (default=None).\n",
    "           搜寻最佳划分的时候考虑的特征数量。\n",
    "           (1).如果为整数，每次分裂只考虑max_features个特征。\n",
    "           (2).如果为浮点数(0到1之间)，每次切分只考虑int(max_features * n_features)个特征。\n",
    "           (3).如果为'auto'或者'sqrt',则每次切分只考虑sqrt(n_features)个特征\n",
    "           (4).如果为'log2',则每次切分只考虑log2(n_features)个特征。\n",
    "           (5).如果为None,则每次切分考虑n_features个特征。\n",
    "           (6).如果已经考虑了max_features个特征，但还是没有找到一个有效的切分，那么还会继续寻找\n",
    "           下一个特征，直到找到一个有效的切分为止。\n",
    "8.random_state:int, RandomState instance or None, optional (default=None)\n",
    "           (1).如果为整数，则它指定了随机数生成器的种子。\n",
    "           (2).如果为RandomState实例，则指定了随机数生成器。\n",
    "           (3).如果为None，则使用默认的随机数生成器。\n",
    "9.max_leaf_nodes: int or None, optional (default=None)。指定了叶子节点的最大数量。\n",
    "           (1).如果为None,叶子节点数量不限。\n",
    "           (2).如果为整数，则max_depth被忽略。\n",
    "10.min_impurity_decrease:float, optional (default=0.)\n",
    "         如果节点的分裂导致不纯度的减少(分裂后样本比分裂前更加纯净)大于或等于min_impurity_decrease，则分裂该节点。\n",
    "         加权不纯度的减少量计算公式为：\n",
    "         min_impurity_decrease=N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                            - N_t_L / N_t * left_impurity)\n",
    "         其中N是样本的总数，N_t是当前节点的样本数，N_t_L是分裂后左子节点的样本数，\n",
    "         N_t_R是分裂后右子节点的样本数。impurity指当前节点的基尼指数，right_impurity指\n",
    "         分裂后右子节点的基尼指数。left_impurity指分裂后左子节点的基尼指数。\n",
    "11.min_impurity_split:float\n",
    "         树生长过程中早停止的阈值。如果当前节点的不纯度高于阈值，节点将分裂，否则它是叶子节点。\n",
    "         这个参数已经被弃用。用min_impurity_decrease代替了min_impurity_split。\n",
    "12.class_weight:dict, list of dicts, \"balanced\" or None, default=None\n",
    "         类别权重的形式为{class_label: weight}\n",
    "         (1).如果没有给出每个类别的权重，则每个类别的权重都为1。\n",
    "         (2).如果class_weight='balanced'，则分类的权重与样本中每个类别出现的频率成反比。\n",
    "         计算公式为：n_samples / (n_classes * np.bincount(y))\n",
    "         (3).如果sample_weight提供了样本权重(由fit方法提供)，则这些权重都会乘以sample_weight。\n",
    "13.presort:bool, optional (default=False)\n",
    "        指定是否需要提前排序数据从而加速训练中寻找最优切分的过程。设置为True时，对于大数据集\n",
    "        会减慢总体的训练过程；但是对于一个小数据集或者设定了最大深度的情况下，会加速训练过程。\n",
    "属性:\n",
    "1.classes_:array of shape = [n_classes] or a list of such arrays\n",
    "        类别的标签值。\n",
    "2.feature_importances_ : array of shape = [n_features]\n",
    "        特征重要性。越高，特征越重要。\n",
    "        特征的重要性为该特征导致的评价准则的（标准化的）总减少量。它也被称为基尼的重要性\n",
    "3.max_features_ : int\n",
    "        max_features的推断值。\n",
    "4.n_classes_ : int or list\n",
    "        类别的数量\n",
    "5.n_features_ : int\n",
    "        执行fit后，特征的数量\n",
    "6.n_outputs_ : int\n",
    "        执行fit后，输出的数量\n",
    "7.tree_ : Tree object\n",
    "        树对象，即底层的决策树。\n",
    "方法:\n",
    "1.fit(X,y):训练模型。\n",
    "2.predict(X):预测\n",
    "3.predict_log_poba(X):预测X为各个类别的概率对数值。\n",
    "4.predict_proba(X):预测X为各个类别的概率值。\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
